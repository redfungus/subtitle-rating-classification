{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from babelfish import *\n",
    "from subliminal import *\n",
    "from imdb import IMDb\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.context import SparkContext\n",
    "import pysrt\n",
    "import pickle\n",
    "\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping functions for extracting the age rating\n",
    "\n",
    "These maps are used to map the movie string to the Swedish rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_name_to_id(movie):\n",
    "    from imdb import IMDb\n",
    "    ia = IMDb()\n",
    "    return ia.search_movie(movie)[0].getID()\n",
    "    \n",
    "def id_to_parents_guide(movie_id):\n",
    "    from imdb import IMDb\n",
    "    ia = IMDb()\n",
    "    return ia.get_movie_parents_guide(movie_id)['data']['certification']\n",
    "\n",
    "def get_swedish_rating(ratings):\n",
    "    for rating in ratings:\n",
    "        if \"Sweden\" in rating:\n",
    "            return rating.split(':')[1]\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map movie name to subtitle vector\n",
    "\n",
    "These maps are used to map the movie name to vectors used in the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_for_subtitle(movie_name):\n",
    "    video = Video.fromname(movie_name)\n",
    "    return video\n",
    "    \n",
    "def map_to_subtitle(video):\n",
    "    subtitles = list_subtitles([video], {Language('eng')})\n",
    "    subtitle = subtitles[video][0]\n",
    "    download_subtitles([subtitle])\n",
    "    return subtitle.text\n",
    "\n",
    "def trim_srt(subtitle_text):\n",
    "    import pysrt\n",
    "    return pysrt.from_string(subtitle_text).text\n",
    "\n",
    "def sub_to_vec(subtitle):\n",
    "    from sklearn.feature_extraction.text import HashingVectorizer\n",
    "    vectorizer = HashingVectorizer(n_features=2**4)\n",
    "    X = vectorizer.fit_transform([subtitle])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping procedure\n",
    "\n",
    "The movies in the `movie_list` array are used for gathering training data. Some movies are not available in the API and they might result in an error in that case another movie name should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_list = ['matrix', 'lion king', 'pulp fiction', 'titanic', 'toy story', 'deadpool', 'bad boys']\n",
    "\n",
    "ia = IMDb()\n",
    "\n",
    "movie_list_parallel = sc.parallelize(movie_list)\n",
    "movie_ids = movie_list_parallel.map(lambda x: (movie_name_to_id(x), get_movie_for_subtitle(x)))\n",
    "movie_parent_guides = movie_ids.map(lambda x: (id_to_parents_guide(x[0]), x[1]))\n",
    "sweden_rating = movie_parent_guides.map(lambda x: (get_swedish_rating(x[0]), map_to_subtitle(x[1])))\n",
    "sweden_rating_clean = sweden_rating.map(lambda x: (x[0], trim_srt(x[1])))\n",
    "sweden_rating_vectorized = sweden_rating_clean.map(lambda x: (x[0], sub_to_vec(x[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformation\n",
    "\n",
    "Collecting the data in the master row and transforming it to data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweden_rating_vectorized.persist()\n",
    "data = sweden_rating_vectorized.collect()\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for row in data:\n",
    "    X.append(row[1].toarray()[0])\n",
    "    y.append(int(row[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "In this section we train a simple `RandomForestRegressor` using the data collected in the previous steps. \n",
    "\n",
    "It sould be noted that the training part is not the main part of the project and the model can be provided from any external source. The current model might not be the most optimized model for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                      n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                      warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "regr = RandomForestRegressor(max_depth=2, random_state=0,\n",
    "                              n_estimators=100)\n",
    "regr.fit(X, y)  \n",
    "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
    "           max_features='auto', max_leaf_nodes=None,\n",
    "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "           min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "           oob_score=False, random_state=0, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "A simple example to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.92]\n"
     ]
    }
   ],
   "source": [
    "test_str = 'uncle'\n",
    "\n",
    "vector = sub_to_vec(test_str).toarray()[0]\n",
    "\n",
    "print(regr.predict(vector.reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model\n",
    "\n",
    "In this cell we save the model to be used for the next part of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model.sav'\n",
    "pickle.dump(regr, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
